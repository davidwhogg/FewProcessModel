{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidwhogg/FewProcessModel/blob/main/notebooks/Hogg_for_Griffith.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ut_pYHbZN0_"
      },
      "source": [
        "# A two-process (or `K`-process) model for stellar abundances\n",
        "\n",
        "## Authors:\n",
        "- **Emily J. Griffith** (Colorado)\n",
        "- **David W. Hogg** (NYU) (MPIA) (Flatiron)\n",
        "\n",
        "## Things to discuss:\n",
        "- Note that the model becomes ambiguous at high metallicity (maybe because there are no longer two sequences!). Fix this somehow? Maybe by putting all of high metallicity into a wider bin? Or perhaps by penalizing derivatives dq/dZ? Observation is: When we use the fine `EJG` binning we get craziness at high Z; when we use the coarser `DWH` binning we don't? Maybe? Or maybe it's sufficient to extrapolate outside the range with the linear dependence inside the range? A lot of this can be fixed with the knot formalism proposed in the Major bugs list.\n",
        "- Right now the code de-emphasizes element measurements that are outside a certain, hard box. That's wrong; and what should we do instead of that to deal with outliers?\n",
        "- How to display results and/or what plots are most diagnostic? How to best show dependences on `q_CC_Fe`?\n",
        "\n",
        "## Major bugs / to-do items:\n",
        "- When we want to go to `K=4` I now think that we have to optimize `K=2` and then add a process, optimize, and then add another, and then optimize again. That requires some refactoring.\n",
        "- Right now, the `bins` and `metallicities` inputs are used only for plotting? Maybe we should drop them entirely?\n",
        "- Set up the notebook so it doesn't go from scratch by default; it can save pickle files at checkpoints and restart there instead. Move to operating on local hardware rather than the Colab.\n",
        "- Consider de-emphasizing some elements in the fit when we run the A-step or looking at which elements we want to use / believe for the A-step.\n",
        "- The violence we do to the uncertainties / inverse variances right after reading the data is terrible; fix this.\n",
        "- We ought to produce some kind of error estimates on everything for which we claim results.\n",
        "- We should go to a wider metallicity range, and non-uniform metallicity bins.\n",
        "- Maybe regularize the CCSN process to be more dominate at low metallicity when it doesn't know what process to assign abundances to\n",
        "\n",
        "## Minor bugs / stretch goals:\n",
        "- Should we make versions of the element-element plots in which we add in fake noise so that the model looks more like the data?\n",
        "- `Aq-step()` contains a lot of optimization hacks of which we are not proud.\n",
        "- We could add a \"jitter\" term to the observational errors on the abundances?\n",
        "- Shouldn't be taking a `sqrt` in the residual (chi) code.\n",
        "\n",
        "## Comments\n",
        "- `jax vmap()` completely changed what was possible in this project. We must acknowledge & cite them in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dudptFAxA8HX"
      },
      "outputs": [],
      "source": [
        "!pip install jaxopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC2f4glXFDVi"
      },
      "outputs": [],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJx9Reo4ZIc0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from jax.scipy.special import logsumexp\n",
        "import jaxopt\n",
        "import wget\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import LogNorm\n",
        "import os.path\n",
        "from tqdm import tqdm\n",
        "from jax import vmap, grad, jit\n",
        "from functools import partial\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiGBUqkO0Sv9"
      },
      "outputs": [],
      "source": [
        "from jax.config import config\n",
        "config.update(\"jax_enable_x64\", True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFIYs_vo6kh2"
      },
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(42) # for all important random numbers\n",
        "rng2 = np.random.default_rng(17) # for random numbers used just in plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J8hKKU4MDWj"
      },
      "outputs": [],
      "source": [
        "# Revise default plotting style\n",
        "style_revisions = {\n",
        "            'axes.linewidth': 1.5,\n",
        "            'xtick.top' : True, \n",
        "            'ytick.right' : True, \n",
        "            'xtick.direction' : 'in',\n",
        "            'ytick.direction' : 'in', \n",
        "            'xtick.major.size' : 11, \n",
        "            'ytick.major.size' : 11, \n",
        "            'xtick.minor.size' : 5.5, \n",
        "            'ytick.minor.size' : 5.5,\n",
        "            'font.size' : 16,\n",
        "            'figure.figsize' : [6, 6],\n",
        "            'lines.linewidth' : 2.5,\n",
        "        }\n",
        "plt.rcParams.update(style_revisions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtqPZE-bdSum"
      },
      "outputs": [],
      "source": [
        "# Set constants\n",
        "ln10 = np.log(10.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7beq-jhEZSm7"
      },
      "outputs": [],
      "source": [
        "# Set hyper-parameters\n",
        "K = 2              # number of processes\n",
        "processes = np.array(['CC', 'Ia', 'AGB', 'fourth']) # process names\n",
        "processes = processes[:K]\n",
        "Lambda_a = 1.e6    # regularization strength on Mg for CC and on Fe for Ia.\n",
        "Lambda_b = 1.e6    # regularization strength on Mg for Ia and Fe for CC.\n",
        "q_CC_Fe = 0.35     # q_CC,Fe at Z=0\n",
        "dq_CC_Fe_dZ = 0.0  # slope wrt Z\n",
        "Lambda_c = 1.0     # regularization strength on everything else in q\n",
        "Lambda_d = 1.e3    # regularization strength on the A values for processes 3+\n",
        "sqrt_Lambda_A = jnp.ones(K) * jnp.sqrt(Lambda_d) # see way above\n",
        "sqrt_Lambda_A = jnp.where(processes == \"CC\", 0., sqrt_Lambda_A)\n",
        "sqrt_Lambda_A = jnp.where(processes == \"Ia\", 0., sqrt_Lambda_A)\n",
        "def get_A_regularization():\n",
        "    return sqrt_Lambda_A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLMEvU-qCImw"
      },
      "outputs": [],
      "source": [
        "# Download files from Emily's website\n",
        "url = 'https://www.emilyjgriffith.com/s/'\n",
        "if(os.path.isfile('lnqs.npy')==False): wget.download(url+'lnqs.npy')\n",
        "if(os.path.isfile('lnAs.npy')==False): wget.download(url+'lnAs.npy')\n",
        "if(os.path.isfile('bins.npy')==False): wget.download(url+'bins.npy')\n",
        "if(os.path.isfile('alldata.npy')==False): wget.download(url+'alldata.npy')\n",
        "if(os.path.isfile('allivars.npy')==False): wget.download(url+'allivars.npy')\n",
        "\n",
        "# These are the new files that extend to lower Z\n",
        "# Emily commented out bins files so that we rememeber to recreate\n",
        "\n",
        "#if(os.path.isfile('bins_2.npy')==False): wget.download(url+'bins_2.npy')\n",
        "if(os.path.isfile('alldata_2.npy')==False): wget.download(url+'alldata_2.npy')\n",
        "if(os.path.isfile('allivars_2.npy')==False): wget.download(url+'allivars_2.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C14zrTA5F5wP"
      },
      "outputs": [],
      "source": [
        "# Load numpy files\n",
        "# N - number of stars = 34410\n",
        "# M - number of elements = 16\n",
        "\n",
        "# warning: MAGIC; BRITTLE\n",
        "elements  = np.array(['Mg','O','Si','S','Ca','CN','Na','Al','K','Cr','Fe','Ni','V','Mn','Co','Ce'])\n",
        "M = len(elements)\n",
        "\n",
        "# lnqs: shape(2, 12, 16), 0 is qcc, 1 is qIa, replaced negative values with 0.05\n",
        "w22_lnqs = np.load('lnqs.npy')\n",
        "w22_metallicity_labels = np.array(['-0.7', '-0.6', '-0.5', '-0.4', '-0.3',\n",
        "                                   '-0.2', '-0.1', '0.0', '0.1', '0.2', '0.3',\n",
        "                                   '0.4', ])\n",
        "w22_metallicities = np.array([float(m) for m in w22_metallicity_labels])\n",
        "a, b, c = w22_lnqs.shape\n",
        "assert b == len(w22_metallicities)\n",
        "assert c == M\n",
        "\n",
        "# artificially raise the zeros above zero\n",
        "w22_lnqs = np.clip(w22_lnqs, -7., None)\n",
        "\n",
        "# alldata: shape(34410, 16), bad data = 0\n",
        "alldata = np.load('alldata_2.npy')\n",
        "N, b = alldata.shape\n",
        "assert b == len(elements)\n",
        "\n",
        "# allivars: shape(34410,16), bad data = 0\n",
        "allivars = np.load('allivars_2.npy')\n",
        "assert allivars.shape == alldata.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27-6M_JsN4P2"
      },
      "outputs": [],
      "source": [
        "# define the binning of the data MAGIC\n",
        "# THIS IS NOW DEPRECATED -- IT IS ONLY USED IN PLOTTING.\n",
        "EJG_bin_edges = np.array([-2.2, -1.7, -1.5, -1.3, -1.1, -0.95, -0.80, -0.65, -0.55, \n",
        "             -0.45, -0.35, -0.25, -0.15, -0.05,  0.05,  0.15,  0.25,  0.35,\n",
        "             0.45, 0.6])\n",
        "DWH_bin_edges = np.array([-2.2, -1.4, -1.1, -0.80, -0.55, -0.45, -0.35, -0.25, -0.15,\n",
        "                          -0.05,  0.05,  0.15,  0.25, 0.35, 0.6])\n",
        "bin_edges = DWH_bin_edges\n",
        "Nbin = len(bin_edges) - 1\n",
        "bins = (np.digitize(alldata[:, elements == \"Mg\"], bin_edges) - 1).flatten()\n",
        "metallicities = np.zeros(Nbin)\n",
        "for bin in range(Nbin):\n",
        "    metallicities[bin] = np.median(alldata[:, elements == \"Mg\"][bins == bin])\n",
        "    print(bin, bin_edges[bin], metallicities[bin], bin_edges[bin + 1], np.sum(bins == bin))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IFyJxho8IgL"
      },
      "outputs": [],
      "source": [
        "# Set up knots for q values\n",
        "knot_xs = np.array([-2.5, -0.5, -0.4, -0.3, -0.2, -0.1, 0.0, 0.1, 0.2, 0.3, 0.55])\n",
        "ii = 0\n",
        "assert elements[ii] == \"Mg\"\n",
        "xs = alldata[:, ii] # BRITTLE MAGIC\n",
        "Nknot = len(knot_xs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYiZk_-AYOHI"
      },
      "outputs": [],
      "source": [
        "# mess with the uncertainties -- THIS IS TOTAL HACKING!\n",
        "# THIS IS NOT PERMITTED.\n",
        "# NOTE brittle 0 (should be \"Mg\")\n",
        "for el, name in enumerate(elements):\n",
        "    allivars[np.where((alldata - alldata[:, 0][:, None]) < -0.5)] = 1. # UGH HACK MAGIC\n",
        "    allivars[np.where((alldata - alldata[:, 0][:, None]) > 0.2)] = 1. # UGH HACK MAGIC\n",
        "sqrt_allivars = jnp.sqrt(allivars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai1dSIWa4NRE"
      },
      "outputs": [],
      "source": [
        "def get_lnqs(lnqs, knot_xs, xs):\n",
        "    \"\"\"\n",
        "    linear interpolation on vmap?\n",
        "    \"\"\"\n",
        "    return jnp.concatenate([vmap(jnp.interp, in_axes=(None, None, 1),\n",
        "                                 out_axes=(1))(xs, knot_xs, lnqs[k])[None, :, :]\n",
        "                            for k in range(K)], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Przaf7t9VhK"
      },
      "outputs": [],
      "source": [
        "def all_stars_K_process_model(lnAs, lnqs, knot_xs, xs):\n",
        "    \"\"\"\n",
        "    ## inputs\n",
        "    - `lnAs`: shape `(K, N)` natural-logarithmic amplitudes\n",
        "    - `lnqs`: shape `(K, Nknot, M)` natural-logarithmic processes\n",
        "    - `knot_xs`: shape `(Nknot, )` metallicity bin centers\n",
        "    - `xs`: shape `(N, )` abundance data (used to interpolate the `lnqs`)\n",
        "\n",
        "    ## outputs\n",
        "    shape `(M, )` log_10 abundances\n",
        "\n",
        "    ## comments\n",
        "    - Note the `ln10`.\n",
        "    \"\"\"\n",
        "    return logsumexp(lnAs[:, :, None]\n",
        "                     + get_lnqs(lnqs, knot_xs, xs), axis=0) / ln10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTOy4PgEaDMi"
      },
      "outputs": [],
      "source": [
        "def one_star_K_process_model(lnAs, lnqs):\n",
        "    \"\"\"\n",
        "    ## inputs\n",
        "    - `lnAs`: shape `(K,)` natural-logarithmic amplitudes\n",
        "    - `lnqs`: shape `(K, M)` natural-logarithmic processes\n",
        "\n",
        "    ## outputs\n",
        "    shape `(M, )` log_10 abundances\n",
        "\n",
        "    ## comments\n",
        "    - Note the `ln10`.\n",
        "    \"\"\"\n",
        "    return logsumexp(lnAs[:, None] + lnqs, axis=0) / ln10\n",
        "\n",
        "def one_star_chi(lnAs, lnqs, data, sqrt_ivars, sqrt_Lambda):\n",
        "    \"\"\"\n",
        "    ## inputs\n",
        "    - `lnAs`: shape `(K, )` natural-logarithmic amplitudes\n",
        "    - `lnqs`: shape `(K, M)` natural-logarithmic processes\n",
        "    - `data`: shape `(M, )` log_10 abundance measurements\n",
        "    - `sqrt_ivars`: shape `(M, )` inverse errors on the data\n",
        "    - `sqrt_Lambda`: shape `(K, )` regularization strength on As\n",
        "\n",
        "    ## outputs\n",
        "    chi for this one star\n",
        "    \"\"\"\n",
        "    return jnp.concatenate([sqrt_ivars * (data - one_star_K_process_model(lnAs, lnqs)),\n",
        "                            sqrt_Lambda * jnp.exp(lnAs)])\n",
        "\n",
        "def one_star_A_step(lnqs, data, sqrt_ivars, sqrt_Lambda, init):\n",
        "    \"\"\"\n",
        "    ## inputs\n",
        "    - `lnqs`: shape `(K, M)` natural-logarithmic processes\n",
        "    - `data`: shape `(M, )` log_10 abundance measurements\n",
        "    - `sqrt_ivars`: shape `(M, )` inverse errors on the data\n",
        "    - `sqrt_Lambda`: shape `(K, )` regularization\n",
        "    - `init`: shape `(K,)` initial guess for the A vector\n",
        "\n",
        "    ## outputs\n",
        "    shape `(K,)` best-fit natural-logarithmic amplitudes\n",
        "\n",
        "    ## bugs\n",
        "    - Doesn't check the output of the optimizer AT ALL.\n",
        "    - Check out the crazy `maxiter` input!\n",
        "    \"\"\"\n",
        "    solver = jaxopt.GaussNewton(residual_fun=one_star_chi, maxiter=4)\n",
        "    lnAs_init = init.copy()\n",
        "    chi2_init = np.sum(one_star_chi(lnAs_init, lnqs, data, sqrt_ivars, sqrt_Lambda) ** 2)\n",
        "    res = solver.run(lnAs_init, lnqs=lnqs, data=data, sqrt_ivars=sqrt_ivars,\n",
        "                     sqrt_Lambda=sqrt_Lambda)\n",
        "    chi2_res = np.sum(one_star_chi(res.params, lnqs, data, sqrt_ivars, sqrt_Lambda) ** 2)\n",
        "    return res.params, chi2_init - chi2_res\n",
        "\n",
        "def A_step(lnqs, data, sqrt_ivars, knot_xs, xs, old_lnAs):\n",
        "    \"\"\"\n",
        "    ## inputs\n",
        "    - `lnqs`: shape `(K, Nknot, M)` natural-logarithmic processes\n",
        "    - `data`: shape `(N, M)` log_10 abundance measurements\n",
        "    - `sqrt_ivars`: shape `(N, M)` inverse variances on alldata\n",
        "    - `knot_xs`: shape `(Nknot, )` metallicity knot locations\n",
        "    - `xs`: shape `(N, )` metallicities (to use with the knots)\n",
        "    - `old_lnAs`: previous `lnAs`; used for initialization of the optimizer\n",
        "\n",
        "    ## outputs\n",
        "    shape `(K, N)` best-fit natural-logarithmic amplitudes\n",
        "\n",
        "    ## bugs\n",
        "    - Ridiculous post-processing of outputs, with MAGIC numbers.\n",
        "    \"\"\"\n",
        "    N, M = data.shape\n",
        "    K, Nknot, em = lnqs.shape\n",
        "    assert em == M\n",
        "    assert sqrt_ivars.shape == (N, M)\n",
        "    assert knot_xs.shape == (Nknot, )\n",
        "    assert old_lnAs.shape == (K, N)\n",
        "    sqrt_Lambda = get_A_regularization()\n",
        "    new_lnAs, dc2 = vmap(one_star_A_step, in_axes=(1, 0, 0, None, 1),\n",
        "                    out_axes=(1, 0))(get_lnqs(lnqs, knot_xs, xs), data,\n",
        "                                     sqrt_ivars, sqrt_Lambda, old_lnAs)\n",
        "    if not jnp.all(jnp.isfinite(new_lnAs)):\n",
        "        print(\"A-step(): fixing bad elements:\", jnp.sum(jnp.logical_not(jnp.isfinite(new_lnAs))))\n",
        "        new_lnAs = jnp.where(jnp.isfinite(new_lnAs), new_lnAs, old_lnAs)\n",
        "    if np.any(new_lnAs > 2.0): # MAGIC HACK\n",
        "        print(\"A-step(): fixing large elements:\", np.sum(new_lnAs > 2.0), np.max(new_lnAs))\n",
        "        new_lnAs = jnp.where(new_lnAs > 2.0, 2.0, new_lnAs)\n",
        "    if np.any(new_lnAs < -9.0): # MAGIC HACK\n",
        "        print(\"A-step(): fixing small elements:\", np.sum(new_lnAs < -9.0), np.min(new_lnAs))\n",
        "        new_lnAs = jnp.where(new_lnAs < -9.0, -9.0, new_lnAs)\n",
        "    return new_lnAs, dc2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vI2G7ADilwd"
      },
      "outputs": [],
      "source": [
        "def one_element_K_process_model(lnqs, lnAs):\n",
        "    \"\"\"\n",
        "    ## inputs\n",
        "    - `lnqs`: shape `(K, N)` natural-logarithmic process elements\n",
        "    - `lnAs`: shape `(K, N)` natural-logarithmic amplitudes\n",
        "\n",
        "    ## outputs\n",
        "    shape `(N, )` log_10 abundances\n",
        "\n",
        "    ## comments\n",
        "    - Note the `ln10`.\n",
        "    \"\"\"\n",
        "    return logsumexp(lnqs + lnAs, axis=0) / ln10\n",
        "\n",
        "def q_step_regularization(lnqs):\n",
        "    \"\"\"\n",
        "    Build arrays that are used for the regularization of the q step.\n",
        "\n",
        "    ## outputs\n",
        "    `Lambdas, q0s` regularization amplitudes and mean values; same shape as `lnqs`.\n",
        "\n",
        "    ## bugs:\n",
        "    - Depends on many global variables and choices.\n",
        "    \"\"\"\n",
        "    Lambdas = np.zeros_like(lnqs) + Lambda_c # default value\n",
        "    q0s = np.zeros_like(lnqs) + 0.5 # default value\n",
        "    fixed = np.zeros_like(lnqs).astype(bool)\n",
        "\n",
        "    # First point: Strongly require that q_Mg = 1 for CC\n",
        "    elem = elements == \"Mg\"\n",
        "    proc = processes == \"CC\"\n",
        "    Lambdas[proc, :, elem] = Lambda_a\n",
        "    q0s[    proc, :, elem] = 1.0\n",
        "    fixed[  proc, :, elem] = True\n",
        "\n",
        "    # Second point: Strongly require that q_Fe = 0.5 for Ia\n",
        "    elem = elements == \"Fe\"\n",
        "    proc = processes == \"Ia\"\n",
        "    Lambdas[proc, :, elem] = Lambda_a\n",
        "    q0s[    proc, :, elem] = 0.5\n",
        "    fixed[  proc, :, elem] = True\n",
        "\n",
        "    # Third point: Require that q_Mg = 0 for all but CC\n",
        "    elem = elements == \"Mg\"\n",
        "    proc = processes != \"CC\"\n",
        "    Lambdas[proc, :, elem] = Lambda_b\n",
        "    q0s[    proc, :, elem] = 0.0\n",
        "    fixed[  proc, :, elem] = True\n",
        "\n",
        "    # Fourth point: Require that q_Fe has some particular value / form for CC\n",
        "    elem = elements == \"Fe\"\n",
        "    proc = processes == \"CC\"\n",
        "    Lambdas[proc, :, elem] = Lambda_b\n",
        "    q0s[    proc, :, elem] = q_CC_Fe + dq_CC_Fe_dZ * knot_xs\n",
        "    fixed[  proc, :, elem] = True\n",
        "\n",
        "    # Now set the form for any AGB process\n",
        "    elem = elements == \"CN\"\n",
        "    proc = processes == \"AGB\"\n",
        "    Lambdas[proc, :, elem] = Lambda_b\n",
        "    q0s[    proc, :, elem] = 0.5\n",
        "    fixed[  proc, :, elem] = True\n",
        "\n",
        "    # Now set the form for a fourth process\n",
        "    elem = elements == \"Co\"\n",
        "    proc = processes == \"fourth\"\n",
        "    Lambdas[proc, :, elem] = Lambda_b\n",
        "    q0s[    proc, :, elem] = 0.5\n",
        "    fixed[  proc, :, elem] = True\n",
        "\n",
        "    return Lambdas, q0s, fixed\n",
        "\n",
        "def one_element_chi(lnqs, lnAs, data, sqrt_ivars, knot_xs, xs, sqrt_Lambdas, q0s):\n",
        "    \"\"\"\n",
        "    ## inputs\n",
        "    - `lnqs`: shape `(K, Nknot)` natural-logarithmic process vectors\n",
        "    - `lnAs`: shape `(K, N)` natural-logarithmic amplitudes\n",
        "    - `data`: shape `(N, )` log_10 abundance measurements\n",
        "    - `sqrt_ivars`: shape `(N, )` inverse variances on the data\n",
        "    - `knot_xs`: shape `(Nknot, )` metallicity bin \"centers\"\n",
        "    - `xs` : shape `(N, )` metallicities to use with `metallicities`\n",
        "    - `sqrt_Lambdas`: shape `(K, Nbin)` list of regularization amplitudes\n",
        "    - `q0s`: shape `(K, Nknot)` \n",
        "\n",
        "    ## outputs\n",
        "    chi for this one star (weighted residual)\n",
        "    \"\"\"\n",
        "    interp_lnqs = get_lnqs(lnqs[:, :, None], knot_xs, xs)[:, :, 0]\n",
        "    return jnp.concatenate([sqrt_ivars * (data - one_element_K_process_model(interp_lnqs, lnAs)),\n",
        "                            jnp.ravel(sqrt_Lambdas * (jnp.exp(lnqs) - q0s))])\n",
        "\n",
        "def one_element_q_step(lnAs, data, sqrt_ivars, knot_xs, xs, sqrt_Lambdas, q0s,\n",
        "                       fixed, init):\n",
        "    \"\"\"\n",
        "    ## inputs\n",
        "    - `lnAs`: shape `(K, N)` natural-logarithmic amplitudes\n",
        "    - `data`: shape `(N, )` log_10 abundance measurements\n",
        "    - `sqrt_ivars`: shape `(N, )` inverse errors on the data\n",
        "    - `knot_xs`: shape `(Nknot, )` metallicity bin centers\n",
        "    - `xs` : shape `(N, )` metallicities to use with `metallicities`\n",
        "    - ... \n",
        "\n",
        "    ## outputs\n",
        "    shape `(K, Nknot)` best-fit natural-logarithmic process elements\n",
        "\n",
        "    ## bugs\n",
        "    - Uses the `fixed` input incredibly stupidly, because Hogg SUX.\n",
        "    - Doesn't check the output of the optimizer AT ALL.\n",
        "    - Check out the crazy `maxiter` input!\n",
        "    \"\"\"\n",
        "    solver = jaxopt.GaussNewton(residual_fun=one_element_chi, maxiter=4)\n",
        "    lnqs_init = init.copy()\n",
        "    chi2_init = np.sum(one_element_chi(lnqs_init, lnAs, data, sqrt_ivars, \n",
        "                       knot_xs, xs, sqrt_Lambdas, q0s) ** 2)\n",
        "    res = solver.run(lnqs_init, lnAs=lnAs, data=data, sqrt_ivars=sqrt_ivars,\n",
        "                     knot_xs=knot_xs, xs=xs,\n",
        "                     sqrt_Lambdas=sqrt_Lambdas, q0s=q0s)\n",
        "    chi2_res = np.sum(one_element_chi(res.params, lnAs, data, sqrt_ivars, \n",
        "                      knot_xs, xs, sqrt_Lambdas, q0s) ** 2)\n",
        "    return jnp.where(fixed, lnqs_init, res.params), chi2_init - chi2_res\n",
        "\n",
        "def q_step(lnAs, data, sqrt_ivars, knot_xs, xs, old_lnqs):\n",
        "    \"\"\"\n",
        "    ## inputs\n",
        "    - `lnAs`: shape `(K, N)` natural-logarithmic amplitudes\n",
        "    - `alldata`: shape `(N, M)` log_10 abundance measurements\n",
        "    - `sqrt_allivars`: shape `(N, M)` inverse errors on alldata\n",
        "    - `knot_xs`: shape `(Nknot, )` metallicity bin centers\n",
        "    - `xs` : shape `(N, )` metallicities to use with `metallicities`\n",
        "    - `old_lnqs`: shape `(K, Nbin, M)` initialization for optimizations\n",
        "\n",
        "    ## outputs\n",
        "    shape `(K, Nbin, M)` best-fit natural-logarithmic processes\n",
        "\n",
        "    ## bugs\n",
        "    - Ridiculous post-processing of outputs.\n",
        "    \"\"\"\n",
        "    N, M = data.shape\n",
        "    assert lnAs.shape == (K, N)\n",
        "    assert sqrt_ivars.shape == (N, M)\n",
        "    Nknot = len(knot_xs)\n",
        "    assert len(xs) == N\n",
        "    assert old_lnqs.shape == (K, Nknot, M)\n",
        "    lnqs1 = np.zeros((K, Nknot, M))\n",
        "    Lambdas, q0s, fixed = q_step_regularization(old_lnqs)\n",
        "    new_lnqs, dc2 = vmap(one_element_q_step, in_axes=(None, 1, 1, None, None, 2, 2, 2, 2),\n",
        "                    out_axes=(2, 0))(lnAs, data, sqrt_ivars, knot_xs, xs,\n",
        "                                jnp.sqrt(Lambdas), jnp.array(q0s),\n",
        "                                jnp.array(fixed), old_lnqs)\n",
        "    if not np.all(jnp.isfinite(new_lnqs)):\n",
        "        print(\"q-step(): fixing bad elements:\", np.sum(jnp.logical_not(jnp.isfinite(new_lnqs))))\n",
        "        new_lnqs = jnp.where(jnp.isfinite(new_lnqs), new_lnqs, old_lnqs)\n",
        "    if np.any(new_lnqs > 1.0): # MAGIC HACK\n",
        "        print(\"q-step(): fixing large elements:\", np.sum(new_lnqs > 1.0), np.max(new_lnqs))\n",
        "        new_lnqs = jnp.where(new_lnqs > 1.0, 1.0, new_lnqs)\n",
        "    if np.any(new_lnqs < -9.0): # MAGIC HACK\n",
        "        print(\"q-step(): fixing small elements:\", np.sum(new_lnqs < -9.0), np.min(new_lnqs))\n",
        "        new_lnqs = jnp.where(new_lnqs < -9.0, -9.0, new_lnqs)\n",
        "    return new_lnqs, dc2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfb7cXfLkSj8"
      },
      "outputs": [],
      "source": [
        "def objective_q(lnAs, lnqs, data, sqrt_ivars, knot_xs, xs):\n",
        "    \"\"\"\n",
        "    This is NOT the objective, but it stands in for now!!\n",
        "    \"\"\"\n",
        "    Lambdas, q0s, _ = q_step_regularization(lnqs)\n",
        "    chi = vmap(one_element_chi, in_axes=(2, None, 1, 1, None, None, 2, 2),\n",
        "               out_axes=(0))(lnqs, lnAs, data, sqrt_ivars, knot_xs, xs,\n",
        "                             jnp.sqrt(Lambdas), q0s)\n",
        "    sqrt_Lambda = get_A_regularization()\n",
        "    return np.sum(chi * chi) + np.sum((sqrt_Lambda[:, None] * jnp.exp(lnAs)) ** 2)\n",
        "\n",
        "def objective_A(lnAs, lnqs, data, sqrt_ivars, knot_xs, xs):\n",
        "    \"\"\"\n",
        "    This is NOT the objective, but it stands in for now!!\n",
        "    \"\"\"\n",
        "    sqrt_Lambda = get_A_regularization()\n",
        "    chi = vmap(one_star_chi, in_axes=(1, 1, 0, 0, None),\n",
        "               out_axes=(0))(lnAs, get_lnqs(lnqs, knot_xs, xs),\n",
        "                             data, sqrt_ivars, sqrt_Lambda)\n",
        "    Lambdas, q0s, _ = q_step_regularization(lnqs)\n",
        "    return np.sum(chi ** 2) + np.sum(Lambdas * (jnp.exp(lnqs) - q0s) ** 2)\n",
        "\n",
        "def Aq_step(data, sqrt_ivars, knot_xs, xs, ln_noise, old_lnAs, old_lnqs, rng=rng):\n",
        "    \"\"\"\n",
        "    ## Bugs:\n",
        "    - This contains multiple hacks.\n",
        "    - Maybe some of the hacks should be pushed back into the A-step and\n",
        "      the q-step?\n",
        "    \"\"\"\n",
        "    prefix = \"Aq-step():\"\n",
        "    old_objective = objective_A(old_lnAs, old_lnqs, data, sqrt_ivars, knot_xs, xs)\n",
        "\n",
        "    # fix old_lnAs\n",
        "    old_lnAs = jnp.where(jnp.isnan(old_lnAs), 1., old_lnAs)\n",
        "\n",
        "    # add noise\n",
        "    A_noise = ln_noise + np.log(rng.uniform(size=old_lnAs.shape))\n",
        "    init_lnAs = jnp.logaddexp(old_lnAs, A_noise)\n",
        "    q_noise = ln_noise + np.log(rng.uniform(size=old_lnqs.shape))\n",
        "    q_noise[:, :, elements == \"Mg\"] = -np.inf # HACK \n",
        "    q_noise[:, :, elements == \"Fe\"] = -np.inf # HACK\n",
        "    init_lnqs = jnp.logaddexp(old_lnqs, q_noise)\n",
        "\n",
        "    # run q step\n",
        "    objective1 = objective_q(init_lnAs, old_lnqs, data, sqrt_ivars, knot_xs, xs)\n",
        "    new_lnqs, _ = q_step(init_lnAs, data, sqrt_ivars, knot_xs, xs, old_lnqs)\n",
        "    objective2 = objective_q(init_lnAs, new_lnqs, data, sqrt_ivars, knot_xs, xs)\n",
        "    if objective2 > objective1:\n",
        "        print(prefix, \"q-step WARNING: objective function got worse:\", objective1, objective2)\n",
        "        new_lnqs = old_lnqs.copy()\n",
        "        objective2 = objective1\n",
        "\n",
        "    # run A step\n",
        "    objective3 = objective_A(init_lnAs, new_lnqs, data, sqrt_ivars, knot_xs, xs)\n",
        "    new_lnAs, _ = A_step(new_lnqs, data, sqrt_ivars, knot_xs, xs, init_lnAs)\n",
        "    objective4 = objective_A(new_lnAs, new_lnqs, data, sqrt_ivars, knot_xs, xs)\n",
        "    if objective4 > objective3:\n",
        "        print(prefix, \"A-step WARNING: objective function got worse:\", objective3, objective4)\n",
        "        new_lnAs = init_lnAs.copy()\n",
        "        objective4 = objective3\n",
        "\n",
        "    # check objective\n",
        "    print(old_objective, objective1, objective2, objective3, objective4)\n",
        "    if objective4 < old_objective:\n",
        "        print(prefix, \"we took a step!\", ln_noise, objective4, old_objective - objective4)\n",
        "        return new_lnAs, new_lnqs, np.around(ln_noise + 0.1, 1)\n",
        "    else:\n",
        "        print(prefix, \"we didn't take a step :(\", ln_noise, old_objective, old_objective - objective4)\n",
        "        return old_lnAs.copy(), old_lnqs.copy(), np.around(ln_noise - 1.0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOr_KqX-F1FO"
      },
      "outputs": [],
      "source": [
        "def initialize():\n",
        "    \"\"\"\n",
        "    ## Bugs:\n",
        "    - DOESN'T WORK for K > 2 ??\n",
        "    - very brittle\n",
        "    - relies on global variables\n",
        "    \"\"\"\n",
        "    lnqs = np.zeros((K, Nknot, M))\n",
        "    lnAs = np.zeros((K, N))\n",
        "    _, q0s, fixed = q_step_regularization(lnqs)\n",
        "    lnq0s = np.log(np.clip(q0s, 1.e-7, None))\n",
        "    lnqs = np.zeros_like(lnq0s)\n",
        "    lnqs = jnp.where(fixed, lnq0s, lnqs)\n",
        "    I = [0, 10, 5, 14] # BRITTLE HACK\n",
        "    I = I[:K]\n",
        "    lnAs, _ = A_step(lnqs[:, :, I], alldata[:, I], sqrt_allivars[:, I],\n",
        "                     knot_xs, xs, lnAs)\n",
        "    print(\"initialize():\", np.median(lnAs[1:] - lnAs[0], axis=1))\n",
        "    lnqs, _ = q_step(lnAs, alldata, sqrt_allivars, knot_xs, xs, lnqs)\n",
        "    lnAs, _ = A_step(lnqs, alldata, sqrt_allivars, knot_xs, xs, lnAs)\n",
        "    return lnAs, lnqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIeaKLA8z-WC"
      },
      "outputs": [],
      "source": [
        "ln_noise = -4.\n",
        "new_lnAs, new_lnqs = initialize()\n",
        "for i in range(16):\n",
        "    new_lnAs, new_lnqs, ln_noise = Aq_step(alldata, sqrt_allivars,\n",
        "                                           knot_xs, xs, ln_noise, new_lnAs,\n",
        "                                           new_lnqs)\n",
        "    print(i + 1, np.median(new_lnAs[1:] - new_lnAs[0], axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqjvnWZu_2p3"
      },
      "outputs": [],
      "source": [
        "# Now do one more round of optimization\n",
        "ln_noise = -4.\n",
        "for i in range(16):\n",
        "    new_lnAs, new_lnqs, ln_noise = Aq_step(alldata, sqrt_allivars, knot_xs, xs,\n",
        "                                           ln_noise, new_lnAs, new_lnqs)\n",
        "    print(i + 1, np.median(new_lnAs[1:] - new_lnAs[0], axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GrOqBiUxhEG"
      },
      "outputs": [],
      "source": [
        "def plot_qs(lnqs):\n",
        "    \"\"\"\n",
        "    # Bugs:\n",
        "    - Relies on many global variables.\n",
        "    - Assumes a rigid structure for the processes?\n",
        "    \"\"\"\n",
        "    MgH = np.linspace(np.min(knot_xs), np.max(knot_xs), 300) # plotting xs\n",
        "    new_qs = np.exp(get_lnqs(lnqs, knot_xs, MgH)) # interp to plotting xs\n",
        "    w22_MgH = w22_metallicities\n",
        "    w22_qs = np.exp(w22_lnqs)\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(16):\n",
        "        plt.subplot(4,4,i+1)\n",
        "        new_qcc = new_qs[0,:,i]\n",
        "        new_qIa = new_qs[1,:,i]\n",
        "        w22_qcc = w22_qs[0,:,i]\n",
        "        w22_qIa = w22_qs[1,:,i]\n",
        "\n",
        "        plt.plot(w22_MgH, w22_qcc, 'b-', lw=4, alpha=0.25, label='qcc W22')\n",
        "        plt.plot(w22_MgH, w22_qIa, 'r-', lw=4, alpha=0.25, label='qIa W22')\n",
        "\n",
        "        plt.plot(MgH, new_qcc, 'b-', alpha=0.9, label='qcc new')\n",
        "        plt.plot(MgH, new_qIa, 'r-', alpha=0.9, label='qIa new')\n",
        "\n",
        "        plt.xlabel('[Mg/H]')\n",
        "        plt.xlim(np.min(knot_xs), np.max(knot_xs))\n",
        "        plt.ylabel('q '+elements[i])\n",
        "        plt.ylim(-0.15, 1.5)\n",
        "\n",
        "        if i==0:\n",
        "            plt.legend(ncol=1, fontsize=10)\n",
        "        #plt.ylim(-0.1,1.1)\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c2Zpngn1ubu"
      },
      "outputs": [],
      "source": [
        "plot_qs(new_lnqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUt7JBgZLN24"
      },
      "outputs": [],
      "source": [
        "def plot_model_abundances(lnAs, lnqs, knot_xs, xs, data, sqrt_ivars, noise=False):\n",
        "    \"\"\"\n",
        "    ## bugs:\n",
        "    - Relies on lots of global variables.\n",
        "    \"\"\"\n",
        "    MgHmin = -1.2\n",
        "\n",
        "    synthdata = all_stars_K_process_model(lnAs, lnqs, knot_xs, xs)\n",
        "    synthnoise = 0.\n",
        "    noisestr = \"\"\n",
        "    if noise:\n",
        "        synthnoise = rng2.normal(size=synthdata.shape) / sqrt_ivars\n",
        "        noisestr = \" + noise\"\n",
        "    fig, axes = plt.subplots(len(elements) - 1, 3, figsize=(12,3 * (len(elements) - 1)))\n",
        "\n",
        "    for j in range(len(elements) - 1):\n",
        "        ax = axes[j, 0]\n",
        "        ax.hist2d(data[:,0], data[:,j+1] - data[:,0],\n",
        "                  cmap='magma', bins=100, range=[[MgHmin,0.4],[-0.5,0.2]], norm=LogNorm())\n",
        "        ax.set_xlabel('[Mg/H]')\n",
        "        ax.set_ylabel('[{}/Mg]'.format(elements[j+1]))\n",
        "        ax.set_ylim(-0.5,0.2)\n",
        "        if j == 0:\n",
        "            ax.set_title('observed')\n",
        "\n",
        "        ax = axes[j, 1]\n",
        "        sata = synthdata + synthnoise\n",
        "        ax.hist2d(sata[:,0], sata[:,j+1] - sata[:,0],\n",
        "                  cmap='magma', bins=100, range=[[MgHmin,0.4],[-0.5,0.2]], norm=LogNorm())\n",
        "        ax.set_xlabel('[Mg/H]')\n",
        "        ax.set_ylabel('[{}/Mg]'.format(elements[j+1]))\n",
        "        ax.set_ylim(-0.5,0.2)\n",
        "        if j == 0:\n",
        "            ax.set_title('predicted' + noisestr)\n",
        "\n",
        "        ax = axes[j, 2]\n",
        "        ax.hist2d(sqrt_ivars[:, 0] * (data[:, 0] - synthdata[:, 0]),\n",
        "                  sqrt_ivars[:, j+1] * (data[:, j+1] - synthdata[:, j+1]),\n",
        "                cmap='magma', bins=100, range=[[-10, 10], [-10, 10]], norm=LogNorm())\n",
        "        ax.set_xlabel('[Mg/H] chi')\n",
        "        ax.set_ylabel('[{}/H] chi'.format(elements[j+1]))\n",
        "        if j == 0:\n",
        "            ax.set_title('dimensionless residual')\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC2zDRk2aMO_"
      },
      "outputs": [],
      "source": [
        "plot_model_abundances(new_lnAs, new_lnqs, knot_xs, xs, alldata, sqrt_allivars, noise=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuXK4XcTCp47"
      },
      "outputs": [],
      "source": [
        "# PCA the residuals. This code is stupid-slow.\n",
        "allresids = all_stars_K_process_model(new_lnAs, new_lnqs, knot_xs, xs)\n",
        "ss = np.zeros((Nbin, M))\n",
        "vs = np.zeros((Nbin, 2, M))\n",
        "for bin in range(Nbin):\n",
        "    u, s, v = np.linalg.svd(allresids[bins == bin], full_matrices=False)\n",
        "    ss[bin] = s\n",
        "    vs[bin] = v[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vVHyDEcc3y7"
      },
      "outputs": [],
      "source": [
        "NUM_COLORS = M\n",
        "cm = plt.get_cmap('cool')\n",
        "colors =[]\n",
        "for i in range(NUM_COLORS):\n",
        "    color = cm(1.*i/NUM_COLORS)\n",
        "    colors.append(color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ2kp8-gVCzz"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "for bin in np.arange(Nbin)[-1::-1]:\n",
        "    plt.plot(ss[bin] / np.sum(ss[bin]), \"o\", color=colors[bin], alpha=0.5,\n",
        "             label=\"{:5.2f}\".format(metallicities[bin]))\n",
        "    plt.legend(loc=\"upper right\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT8lw2fCWS5-"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "for bin in np.arange(Nbin)[-1::-1]:\n",
        "    v0 = vs[bin, 0]\n",
        "    if np.median(v0) < 0:\n",
        "        v0 *= -1.\n",
        "    plt.plot(vs[bin, 0], \"o\", color=colors[bin], label=\"$Z = {:+5.2f}$\".format(metallicities[bin]))\n",
        "    plt.plot(vs[bin, 0], \"-\", color=colors[bin], alpha=0.5)\n",
        "    plt.xlim(-0.5, 17)\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(range(M))\n",
        "    ax.set_xticklabels(elements)\n",
        "    plt.legend(loc=\"upper right\", fontsize=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36I346qM_ykK"
      },
      "outputs": [],
      "source": [
        "# DON'T GO BELOW THIS LINE.\n",
        "assert False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWaY3xBT6QI3"
      },
      "outputs": [],
      "source": [
        "# Now optimize some models at different values of q_CC_Fe\n",
        "models = []\n",
        "for q_CC_Fe in np.arange(0.20, 0.51, 0.05):\n",
        "    ln_noise = -4.\n",
        "    for i in range(32):\n",
        "        print(\"model:\", q_CC_Fe, \"iteration:\", i)\n",
        "        new_lnAs, new_lnqs, ln_noise = Aq_step(alldata, sqrt_allivars,\n",
        "                                               metallicities, ln_noise,\n",
        "                                               new_lnAs, new_lnqs)\n",
        "    models += [(q_CC_Fe, new_lnqs, new_lnAs)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20UP7lgoKPK5"
      },
      "outputs": [],
      "source": [
        "for q_CC_Fe, new_lnqs, new_lnAs in models:\n",
        "    plot_qs(new_lnqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Uh4Znv0OfTp"
      },
      "outputs": [],
      "source": [
        "newer_models = []\n",
        "for q_CC_Fe, new_lnAs, new_lnQs in models:\n",
        "    ln_noise = -4.\n",
        "    for i in range(32):\n",
        "        print(\"model:\", q_CC_Fe, \"iteration:\", i)\n",
        "        new_lnAs, new_lnqs, ln_noise = Aq_step(alldata, sqrt_allivars,\n",
        "                                               metallicities, ln_noise,\n",
        "                                               new_lnAs, new_lnqs)\n",
        "    newer_models += [(q_CC_Fe, new_lnqs, new_lnAs)]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}